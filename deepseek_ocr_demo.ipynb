# If the you have already installed from requirements.txt, this will just be a no-op.
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install transformers==4.46.3 tokenizers==0.20.3 pillow numpy einops easydict addict img2pdf pymupdf
import os
import torch
from transformers import AutoModel, AutoTokenizer

MODEL_NAME = "deepseek-ai/DeepSeek-OCR"

print("Loading tokenizer and model:", MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModel.from_pretrained(
    MODEL_NAME,
    _attn_implementation="eager",   # avoids flash-attn / CUDA toolkit issues
    trust_remote_code=True,
    use_safetensors=True,
)

if torch.cuda.is_available():
    print("Using GPU")
    model = model.cuda().to(torch.bfloat16)
else:
    print("Using CPU (may be slower)")
    model = model.to(torch.bfloat16)

model = model.eval()


from pathlib import Path

project_root = Path(".").resolve()
image_path = project_root / "data" / "sample_page.jpg"
output_dir = project_root / "outputs"
output_dir.mkdir(exist_ok=True)

prompt = "<image>\n[grounding]Convert the document to markdown."

print("Image path:", image_path)
assert image_path.exists(), f"Image not found: {image_path}"

res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=str(image_path),
    output_path=str(output_dir),
    base_size=1024,
    image_size=640,
    crop_mode=True,
    save_results=True,
    test_compress=True,
)

print("Done. Outputs saved in:", output_dir)
